{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "* Data cleaning is data set specific but there are some common problems\n",
    "    * Missing values\n",
    "    * Duplicates\n",
    "    * Reporting or collection bias or drift\n",
    "    * Reporting or collection errors\n",
    "* Data cleaning is more effective when it is combined with visualization and some preliminary modeling\n",
    "* Decribe all the steps you take to load and clean the data so that you and others can repeat the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise\n",
    "\n",
    "Use your functions or the ones below to load the train data into a dataframe and combine all the narrative fields into one variable: Narrative. Optionally, remove the previous narrative columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages for loading, cleaning, visualization, and analysis\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import string as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I decided to use my own functions that I had made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I have my own function that will pull each year of the train data in a panda df\n",
    "# You pass a directory and it returns a list of pandas dataframes\n",
    "def getListDataFrames(path):\n",
    "    # initialize an empty list\n",
    "    csvs = list()\n",
    "    # Get all the files in the directory\n",
    "    all_files = os.listdir(path)\n",
    "    # Iterate across each of the files in the Data folder\n",
    "    for fileName in all_files:\n",
    "        # Get the full path name\n",
    "        fileToPull = path + fileName\n",
    "        # Read in the data at that location\n",
    "        filesOfInterest = pd.read_csv(fileToPull, index_col = False, low_memory = False)\n",
    "        csvs.append(filesOfInterest)\n",
    "    return csvs\n",
    "\n",
    "# Here is the list of 16 pandas dataframes corresponding to 2001 thru 2016\n",
    "listOfAccidents = getListDataFrames('/Users/mead/Fall2017/DonBrown-DS6001/InClass1/Data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the pandas together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51623, 153)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.concat(listOfAccidents, ignore_index = True)\n",
    "acc_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# Produce a function to add all of the Narratives together\n",
    "# Takes the index of the NARR1 column as input alongside the dataframe of interest. \n",
    "# It outputs the full Narrative column\n",
    "def combineNARR(NARR1index, dataFrme):\n",
    "# Initialize an empty list of all narratives\n",
    "    allAccs = list()\n",
    "    # Iterate across every row\n",
    "    for acc in range(len(dataFrme.index)):\n",
    "        # Initialize an empty list of each row's narrative\n",
    "        fullNarr = list()\n",
    "        # Iterate across every NARR column\n",
    "        for narr in range(15):\n",
    "            index = narr + NARR1index[0]\n",
    "            # Keep track of the value in that NARR column for that row\n",
    "            narrElement = str(dataFrme.iloc[acc, index])\n",
    "            fullNarr.append(narrElement)\n",
    "        allAccs.append(fullNarr)\n",
    "    return allAccs\n",
    "\n",
    "# This is the which function which keeps track of where NARR1 is\n",
    "which = lambda lst:list(np.where(lst)[0])\n",
    "NARR1index = which([name == 'NARR1' for name in acc_df.columns])\n",
    "\n",
    "# Run the function -- output is a list of lists\n",
    "newColumn = combineNARR(NARR1index, acc_df)\n",
    "\n",
    "# 2\n",
    "# Combine all the NARR together for each row. Make sure to remove the NAs\n",
    "Narratives = [''.join([sentence for sentence in NARR if sentence != 'nan']) for NARR in newColumn]\n",
    "# Add this as a new column for the accidents df\n",
    "acc_df['Narrative'] = pd.Series(Narratives, index=acc_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop old NARR columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop old narrative columns\n",
    "narrList = []\n",
    "for i in range(0,15):\n",
    "    a = str(i + 1)\n",
    "    narrList.append('NARR' + a)\n",
    "acc_df.drop(narrList, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "- Why should we remove duplicates?\n",
    "    - Remove duplicates because a duplicate is going to be regarded as more influential than a single point\n",
    "- How should we remove them?\n",
    "    - Search for columns with all duplicate values (unique/duplicate function or something like that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5372\n"
     ]
    }
   ],
   "source": [
    "# Looking for duplicates\n",
    "# I know that there is something specific in the readme about which columns are\n",
    "# duplicates but I just used the narrative since the seems to be good enough\n",
    "print(len(acc_df.loc[acc_df[['Narrative']].duplicated()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are around 5,372 duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the duplicates\n",
    "acc_clean_df = acc_df.drop_duplicates('Narrative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "There are essentially 3 ways to handle missing values:\n",
    "\n",
    "    1. Remove the columns (variables) with missing values\n",
    "    2. Remove the row (observations) with missing values\n",
    "    3. Impute the missing values\n",
    "\n",
    "The choice of which of these to use depends on the problem and the data. If a variable does not seem important to the problem or if it has many missing values, then eliminating it is reasonable. Similarly if the observation appears to not represent the data or if it has many missing values, then eliminating it seems reasonable.\n",
    "Imputation\n",
    "\n",
    "Imputation can be done in may ways. The most common are the following:\n",
    "\n",
    "    1. Replace the missing value with the mean\n",
    "    2. Replace the missing value with the median\n",
    "    3. Replace the missing value with the mode\n",
    "    4. Use k-nn to\n",
    "        1. Replace the missing value with the mean\n",
    "        2. Replace the missing value with the median\n",
    "        3. Replace the missing value with the mode\n",
    " \n",
    "Options 1 and 2 can only be used with numeric or quantitative variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise\n",
    "\n",
    "1. Find out how many missing values we have by column (variable).\n",
    "2. What technique or techniques should we use to handle them?\n",
    "3. What methods from pandas or sklearn can we use for imputation? Why or how would we use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise Answers\n",
    "\n",
    "1. Many of the columns have a large proportion of values that are NAs. By column is calculated below. Across all columns there are 1,788,460 missing values\n",
    "2. We talk above about this above, but the general idea is that for missing values we can either:\n",
    "    1. Remove the column if most of the values are missing\n",
    "    2. Remove the row if it's missing fundamentally important info\n",
    "    3. Use the mean or median for quantitative variables (imputation)\n",
    "    4. Use the mode for categorical variables. (imputation)\n",
    "3. Techniques for imputation\n",
    "    1. Pandas\n",
    "        1. Provides isnull() to specifically return missing values. Has a large numberof uses.\n",
    "        2. Can use fillna(value) to replace all the na's with a particular value. Useful if you know that missing values have some significance in the original dataset\n",
    "        3. fillna(method='pad') let's you use the most recent non-missing value. This is useful for time series when you want to consider the most recently-available value\n",
    "        4. Can also use fillna(acc_df.mean()) to fill the missing values in each column with the mean value. Applications to quantitative variables are immediate. (also use with median and mode)\n",
    "        5. dropna() lets you get entirely rid of columns (or rows) with missing data. Greatif you decide you want no missing data at all.\n",
    "        6. There is an interpolate() method that allows you to do linear interpolation for missing data. This would be good for time series data.\n",
    "    2. sklearn\n",
    "        1. sklearn has an Imputer class which you can initialize and use with a number of different imputation strategies (eg: mean, median, and mode), to replace missing values.\n",
    "        2. You can also build your own imputer class (example below), inheriting from sklearn's TransformerMixin class, which gives you even more control in transforming your data when you run into missing values. In the example below the method was implemented to look at the datatypes of missing values in order to decide whether to do mean, median, or mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a reminder, the size of our panda is: (46251, 139)\n",
      "Here are the missing values by column: \n",
      "ACCAUSE          0\n",
      "ACCDMG           0\n",
      "ACCTRK         113\n",
      "ACCTRKCL       134\n",
      "ADJUNCT1     38918\n",
      "ADJUNCT2     45446\n",
      "ADJUNCT3     45962\n",
      "ALCOHOL      33633\n",
      "AMPM             7\n",
      "AMTRAK       43336\n",
      "BRAKEMEN      4113\n",
      "CABOOSE1         0\n",
      "CABOOSE2         0\n",
      "CARNBR1       3417\n",
      "CARNBR2      41302\n",
      "CARS             0\n",
      "CARSDMG          0\n",
      "CARSHZD          0\n",
      "CASINJ           0\n",
      "CASINJRR         0\n",
      "CASKLD           0\n",
      "CASKLDRR         0\n",
      "CAUSE            0\n",
      "CAUSE2       40558\n",
      "CDTRHR       11636\n",
      "CDTRMIN      16061\n",
      "CNTYCD          14\n",
      "CONDUCTR      2385\n",
      "COUNTY           0\n",
      "CauseCat     46251\n",
      "             ...  \n",
      "SUBDIV       34256\n",
      "TEMP             0\n",
      "TIMEHR           0\n",
      "TIMEMIN          0\n",
      "TONS             0\n",
      "TOTINJ           0\n",
      "TOTKLD           0\n",
      "TRKCLAS        465\n",
      "TRKDMG           0\n",
      "TRKDNSTY     25338\n",
      "TRKNAME        483\n",
      "TRNDIR        2911\n",
      "TRNNBR        6373\n",
      "TRNSPD           0\n",
      "TYPE             0\n",
      "TYPEQ         3404\n",
      "TYPRR           22\n",
      "TYPSPD        3350\n",
      "TYPTRK         422\n",
      "VISIBLTY         0\n",
      "WEATHER          0\n",
      "YEAR             0\n",
      "YEAR4            0\n",
      "adjunct1     46251\n",
      "adjunct2     46251\n",
      "adjunct3     46251\n",
      "mopera       46251\n",
      "signal       46251\n",
      "subdiv       46251\n",
      "Narrative        0\n",
      "Length: 139, dtype: int64\n",
      "And that's a total of 1597278 missing values across all columns\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "print(\"As a reminder, the size of our panda is: \" + str(acc_clean_df.shape))\n",
    "print(\"Here are the missing values by column: \\n\" + str(acc_clean_df.isnull().sum()))\n",
    "print(\"And that's a total of \" + str(acc_clean_df.isnull().sum().sum()) + \" missing values across all columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class imputes the missing values as\n",
    "# (1) most frequent if the variable is categorical \n",
    "# (2) mean if the variable is real (floating point)\n",
    "# (3) median if the variable is an integer \n",
    "\n",
    "# Here is a class that will provide imputation\n",
    "# This is an extension by D.Brown to sveitser, 2014 https://stackoverflow.com/users/469992/sveitser\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        \n",
    "        Columns of dtype floating point are imputed with the mean.\n",
    "\n",
    "        Columns of other types are imputed with median of the column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') \n",
    "                               else X[c].mean() if X[c].dtype == np.dtype('f')\n",
    "                                else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "Categorical variables must be treated differently from quantitative variables. Their effects on the modeling\n",
    "and machine learning depend on correctly coding them for the analysis. \n",
    "\n",
    "Categorical variables can be entered in many diffent ways. Data scientists should inspect the coding \n",
    "of these variables to insure correctness, approrpriateness for modeling, and easy interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise\n",
    "\n",
    "1. Look at the data types for the variables.\n",
    "2. Which variables are categorical? Look at some of their value_counts()\n",
    "3. Which variables are categorical but are coded as integers? Which variables are integers but coded as objects?\n",
    "4. Replace integer values for TYPE with the text labels. Repeat for one other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Exercise Answers\n",
    "\n",
    "1. Shown below. Easy to access.\n",
    "2. I used http://safetydata.fra.dot.gov/OfficeofSafety/publicsite/Newregulation.aspx?doc=accfile_EFFECTIVE_060111.pdf to decide these. So the categorical variables are amtrak, iyr, imo, railroad, iyr2, imo2, rr2, iyr3, imo3, rr3, gxid, year, month, ampm, type, division, station, milepost, state, visibility, weather, typspd, trnnbr, trndir, typeq, equatt, trkname, trkclass, typtrk, rrcar1, carnbr1, loaded1, rrcar2, carnbr2, loaded2, cause, cause2, acccause, acctrk, acctrkcl, stcnty, jointcd, region, typrr, rrdiv, method, year4, county, cntycd, passtrn, ssb1, ssb2, Narrative, rcl, signal, mopera, adjunct1, adjunct2, sdjunct3, subdiv  \n",
    "3. Miscoding (look below for code):\n",
    "    1. Variables coded as integers but are actually categorical:\n",
    "        1. IMO, IMO3, IYR, IYR3, JOINTCD, MONTH, REGION, STATE, TYPE, VISIBLTY, WEATHER, YEAR, YEAR4\n",
    "    2. Variables coded as objects but are actually integers:\n",
    "        1. BRAKEMEN, CONDUCTR, ENGRS, FIREMEN, INCDTNO, INCDTNO2, INCDTNO3\n",
    "4. Done below for TYPE and for WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACCAUSE       object\n",
       "ACCDMG       float64\n",
       "ACCTRK       float64\n",
       "ACCTRKCL      object\n",
       "ADJUNCT1      object\n",
       "ADJUNCT2      object\n",
       "ADJUNCT3      object\n",
       "ALCOHOL      float64\n",
       "AMPM          object\n",
       "AMTRAK        object\n",
       "BRAKEMEN      object\n",
       "CABOOSE1     float64\n",
       "CABOOSE2     float64\n",
       "CARNBR1       object\n",
       "CARNBR2      float64\n",
       "CARS         float64\n",
       "CARSDMG      float64\n",
       "CARSHZD      float64\n",
       "CASINJ       float64\n",
       "CASINJRR     float64\n",
       "CASKLD       float64\n",
       "CASKLDRR     float64\n",
       "CAUSE         object\n",
       "CAUSE2        object\n",
       "CDTRHR       float64\n",
       "CDTRMIN      float64\n",
       "CNTYCD       float64\n",
       "CONDUCTR      object\n",
       "COUNTY        object\n",
       "CauseCat     float64\n",
       "              ...   \n",
       "SUBDIV        object\n",
       "TEMP         float64\n",
       "TIMEHR       float64\n",
       "TIMEMIN      float64\n",
       "TONS         float64\n",
       "TOTINJ       float64\n",
       "TOTKLD       float64\n",
       "TRKCLAS       object\n",
       "TRKDMG       float64\n",
       "TRKDNSTY      object\n",
       "TRKNAME       object\n",
       "TRNDIR       float64\n",
       "TRNNBR        object\n",
       "TRNSPD         int64\n",
       "TYPE           int64\n",
       "TYPEQ         object\n",
       "TYPRR         object\n",
       "TYPSPD        object\n",
       "TYPTRK       float64\n",
       "VISIBLTY       int64\n",
       "WEATHER        int64\n",
       "YEAR           int64\n",
       "YEAR4          int64\n",
       "adjunct1     float64\n",
       "adjunct2     float64\n",
       "adjunct3     float64\n",
       "mopera       float64\n",
       "signal       float64\n",
       "subdiv       float64\n",
       "Narrative     object\n",
       "Length: 139, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "# Just getting data types. Easy\n",
    "acc_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And these are all of their value_counts(): \n",
      "\n",
      "YEAR\n",
      "4     4018\n",
      "5     3885\n",
      "1     3757\n",
      "3     3646\n",
      "6     3637\n",
      "2     3388\n",
      "7     3332\n",
      "8     2904\n",
      "11    2414\n",
      "10    2335\n",
      "9     2317\n",
      "15    2256\n",
      "13    2192\n",
      "14    2112\n",
      "12    2084\n",
      "16    1974\n",
      "Name: YEAR, dtype: int64\n",
      "\n",
      "AMPM\n",
      "PM    23903\n",
      "AM    22341\n",
      "Name: AMPM, dtype: int64\n",
      "\n",
      "DIVISION\n",
      "SYSTEM                  4398\n",
      "CHICAGO                 1419\n",
      "GULF                     812\n",
      "TWIN CITIES              771\n",
      "SPRINGFIELD              744\n",
      "NORTH LITTLE ROCK        664\n",
      "HOUSTON                  633\n",
      "KANSAS                   610\n",
      "TEXAS                    595\n",
      "NORTHWEST                555\n",
      "PORTLAND                 508\n",
      "ROSEVILLE                495\n",
      "ATLANTA                  488\n",
      "POWDER RIVER             466\n",
      "ALBANY                   465\n",
      "CENTRAL                  447\n",
      "JACKSONVILLE             445\n",
      "NEBRASKA                 434\n",
      "NORTH PLATTE             426\n",
      "SAN ANTONIO              408\n",
      "FLORENCE                 408\n",
      "FORT WORTH               404\n",
      "DENVER                   382\n",
      "SOUTHWEST                379\n",
      "DEARBORN                 378\n",
      "LOS ANGELES              377\n",
      "CALIFORNIA               376\n",
      "HARRISBURG               375\n",
      "MAD                      344\n",
      "LIVONIA                  341\n",
      "                        ... \n",
      "HARLINGEN                  1\n",
      "STOUGHTON                  1\n",
      "GREAT LAKES SOUTH          1\n",
      "CORPUS                     1\n",
      "M AND M                    1\n",
      "CONROE                     1\n",
      "SEATTLE                    1\n",
      "ELLAVILLE                  1\n",
      "MCKINNEY                   1\n",
      "RADIBAUGH                  1\n",
      "WINSLOW                    1\n",
      "NORTH LITLE ROCK           1\n",
      "BUSHWICK                   1\n",
      "INDUSTRIAL SPUR            1\n",
      "RED ROCK                   1\n",
      "LEBANON                    1\n",
      "ILL TRANSFER               1\n",
      "OWASSO SUB                 1\n",
      "STONY RIVER                1\n",
      "Texas                      1\n",
      "LAFAYETTE LINE             1\n",
      "NORTH END                  1\n",
      "ST FRANCIS                 1\n",
      "CONWAY SUBDIVISIONS        1\n",
      "WILLARD                    1\n",
      "WARROAD                    1\n",
      "MANCHESTER                 1\n",
      "NORTH PLATE SU             1\n",
      "SEAGRAVES SUBDIVISIO       1\n",
      "TWINB CITIES               1\n",
      "Name: DIVISION, Length: 831, dtype: int64\n",
      "\n",
      "TRNDIR\n",
      "3.0    13608\n",
      "4.0    11864\n",
      "1.0     9274\n",
      "2.0     8594\n",
      "Name: TRNDIR, dtype: int64\n",
      "\n",
      "TYPE\n",
      "Head on collision            29902\n",
      "Other (described in narr)     4861\n",
      "RR Grade Crossing             3786\n",
      "13                            2343\n",
      "Raking collision              2071\n",
      "Explosive-detonation          1196\n",
      "Broken train collision         856\n",
      "Other impacts                  522\n",
      "Side collision                 409\n",
      "Rearend collision              191\n",
      "Hwy-rail crossing               91\n",
      "fire/violent rupture            15\n",
      "Obstruction                      8\n",
      "Name: TYPE, dtype: int64\n",
      "\n",
      "TYPEQ\n",
      "1      16721\n",
      "7      11976\n",
      "1.0     3257\n",
      "7.0     2095\n",
      "6       1692\n",
      "8       1683\n",
      "2       1545\n",
      "5        903\n",
      "3        499\n",
      "A        379\n",
      "2.0      335\n",
      "6.0      295\n",
      "8.0      268\n",
      "9        261\n",
      "4        246\n",
      "5.0      159\n",
      "D        139\n",
      "3.0       97\n",
      "9.0       86\n",
      "C         85\n",
      "B         61\n",
      "4.0       35\n",
      "E         30\n",
      "Name: TYPEQ, dtype: int64\n",
      "\n",
      "TYPTRK\n",
      "2.0    23099\n",
      "1.0    17146\n",
      "4.0     4176\n",
      "3.0     1408\n",
      "Name: TYPTRK, dtype: int64\n",
      "\n",
      "VISIBLTY\n",
      "2    22869\n",
      "4    18800\n",
      "3     2362\n",
      "1     2220\n",
      "Name: VISIBLTY, dtype: int64\n",
      "\n",
      "WEATHER\n",
      "1    30936\n",
      "2    10471\n",
      "3     3212\n",
      "6     1001\n",
      "4      536\n",
      "5       95\n",
      "Name: WEATHER, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "#print(\"There are \" + str(len(acc_df.columns[acc_df.dtypes == 'object'])) + \" variables which are 'object' (so considered categorical.)\")\n",
    "#print(\"These are the variables considered as categorical 'objects': \" + str(acc_df.columns[acc_df.dtypes == 'object']) + \"\\n\")\n",
    "print(\"And these are all of their value_counts(): \\n\")\n",
    "# Code to look at freq of value_counts for each \n",
    "cat_list = acc_clean_df[['YEAR', 'AMPM', 'DIVISION', 'TRNDIR', 'TYPE', 'TYPEQ', 'TYPTRK', 'VISIBLTY', 'WEATHER']].columns#[acc_clean_df.dtypes == 'object']\n",
    "for i in cat_list:\n",
    "    print(str(i) + \"\\n\" + str(acc_clean_df[i].value_counts()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the variables considered as integer : Index([u'DAY', u'DUMMY3', u'HIGHSPD', u'IMO', u'IMO3', u'IYR', u'IYR3',\n",
      "       u'JOINTCD', u'MONTH', u'POSITON1', u'POSITON2', u'REGION', u'STATE',\n",
      "       u'TRNSPD', u'TYPE', u'VISIBLTY', u'WEATHER', u'YEAR', u'YEAR4'],\n",
      "      dtype='object')\n",
      "\n",
      "These are the variables considered as 'objects': Index([u'ACCAUSE', u'ACCTRKCL', u'ADJUNCT1', u'ADJUNCT2', u'ADJUNCT3', u'AMPM',\n",
      "       u'AMTRAK', u'BRAKEMEN', u'CARNBR1', u'CAUSE', u'CAUSE2', u'CONDUCTR',\n",
      "       u'COUNTY', u'DIVISION', u'DUMMY4', u'ENGRS', u'EQATT', u'FIREMEN',\n",
      "       u'GXID', u'INCDTNO', u'INCDTNO2', u'INCDTNO3', u'LOADED1', u'LOADED2',\n",
      "       u'METHOD', u'MILEPOST', u'PASSTRN', u'RAILROAD', u'RR2', u'RR3',\n",
      "       u'RRCAR1', u'RRCAR2', u'RRDIV', u'SSB1', u'SSB2', u'STATION', u'STCNTY',\n",
      "       u'SUBDIV', u'TRKCLAS', u'TRKDNSTY', u'TRKNAME', u'TRNNBR', u'TYPEQ',\n",
      "       u'TYPRR', u'TYPSPD', u'Narrative'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "print(\"These are the variables considered as integer : \" + str(acc_df.columns[acc_df.dtypes == 'int']) + \"\\n\")\n",
    "print(\"These are the variables considered as 'objects': \" + str(acc_df.columns[acc_df.dtypes == 'object']) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types: \n",
      "Derailment                   29902\n",
      "Other impacts                 4861\n",
      "Hwy-rail crossing             3786\n",
      "Other (described in narr)     2343\n",
      "Side collision                2071\n",
      "Obstruction                   1196\n",
      "Raking collision               856\n",
      "fire/violent rupture           522\n",
      "Rearend collision              409\n",
      "Head on collision              191\n",
      "Broken train collision          91\n",
      "Explosive-detonation            15\n",
      "RR Grade Crossing                8\n",
      "Name: TYPE, dtype: int64\n",
      "\n",
      "Weather: \n",
      "clear     30936\n",
      "cloudy    10471\n",
      "rain       3212\n",
      "snow       1001\n",
      "fog         536\n",
      "sleet        95\n",
      "Name: WEATHER, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "# Now to actually go in and make replacements for all of these values \n",
    "# you can go in and use .replace()\n",
    "acc_clean_df[\"TYPE\"] = acc_clean_df['TYPE'].replace(range(1,14),['Derailment', \n",
    "    'Head on collision', 'Rearend collision', 'Side collision', 'Raking collision', \n",
    "    'Broken train collision', 'Hwy-rail crossing', 'RR Grade Crossing', 'Obstruction', \n",
    "    'Explosive-detonation', 'fire/violent rupture', 'Other impacts', \n",
    "    'Other (described in narr)'])\n",
    "# Or you can go in and use a dictionary\n",
    "#map_typeq = {1:'Freight', '1':'Freight'}\n",
    "#acc_clean_df['TYPEQ'] = acc_clean_df.map(map_typeq)\n",
    "print(\"Types: \\n\" + str(acc_clean_df[\"TYPE\"].value_counts()))\n",
    "\n",
    "# Doing the same thing  but for weather\n",
    "acc_clean_df[\"WEATHER\"] = acc_clean_df['WEATHER'].replace(range(1,7),['clear', \n",
    "    'cloudy', 'rain', 'fog', 'sleet', 'snow'])\n",
    "print(\"\\nWeather: \\n\" + str(acc_clean_df[\"WEATHER\"].value_counts()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
